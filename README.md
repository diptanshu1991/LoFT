ğŸª¶ LoFT CLI â€” Lightweight Finetuning + Deployment Toolkit for Custom LLMs

![License](https://img.shields.io/github/license/diptanshu1991/LoFT)
![Python](https://img.shields.io/badge/python-3.10+-blue)
![LoRA](https://img.shields.io/badge/LoRA-compatible-brightgreen)

> ğŸ”§ Customize small language models (1â€“3B) with LoRA adapters  
> ğŸ’» Train, quantize, and run entirely on CPU â€” even an 8GB MacBook  
> ğŸ§± Foundation for an adapter-powered GenAI deployment workflow  

âœ¨ Designed for **developers building local GenAI apps**, not just ML researchers.


---

## ğŸš€ What is LoFT?

**LoFT CLI** is a lightweight, open-source command-line tool that enables:

- âœ… Finetune lightweight LLMs (like TinyLlama) using LoRA
- âœ… Merge adapters into a standalone Hugging Face model
- âœ… Export to GGUF format
- âœ… Quantize to Q4_0 for CPU inference
- âœ… Run the model locally using `llama.cpp`

Everything works **on MacBooks, CPUs, and low-RAM laptops**.

---

## ğŸ¯ Why LoFT Exists

While others focus on training giant models in the cloud, LoFT empowers developers to:

- ğŸ–¥ï¸ Customize open-source models without GPU dependence
- ğŸ”Œ Deploy LLMs fully offline â€” for privacy-first applications
- ğŸ§© Plug in domain-specific LoRA adapters with one command

Coming soon: **LoFT Recipes** â€” ready-to-use adapters + fine-tuning guides for real-world use cases like customer support, legal Q&A, and content summarization.

---

## ğŸ§  TL;DR: Workflow Summary

| Step     | Command         | Output                |
|----------|-----------------|------------------------|
| Finetune | `loft finetune` | LoRA adapters (`.safetensors`) |
| Merge    | `loft merge`    | Merged HF model        |
| Export   | `loft export`   | GGUF (F32/FP16) model  |
| Quantize | `loft quantize` | Q4_0 GGUF model        |
| Chat     | `loft chat`     | Inference CLI (offline) |
---

## ğŸ“¦ Installation

```bash
# Clone the repo
git clone https://github.com/diptanshu1991/LoFT
cd LoFT

# Optional: create a virtual environment
python3 -m venv venv
source venv/bin/activate

# Install in development mode
pip install -e .

#Download Base Model (Optional but Recommended)
python -c "
from transformers import AutoModelForCausalLM, AutoTokenizer
model_id = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'
AutoModelForCausalLM.from_pretrained(model_id)
AutoTokenizer.from_pretrained(model_id)
"
```
#Install Llama.cpp
```
# Clone llama.cpp
git clone https://github.com/ggerganov/llama.cpp

#build the C++ tools
cd llama.cpp
make

```

You now have access to the `loft` CLI.

Install dependencies:

```bash
cd LoFT
pip install -r requirements.txt
```

---

## ğŸ§ª 1. Finetune a Model with LoRA

Uses `peft` with LoRA adapters (in float16/float32). Trains only LoRA layers.

```bash
loft finetune \
  --model_name TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --dataset data/sample_finetune_data.json \
  --output_dir adapter/ \
  --num_train_epochs 2 \
  --gradient_checkpointing
```

âœ… Supports instruction-tuning format

âœ… Works with JSON datasets

âœ… Output is a LoRA adapter folder

---

## ğŸ”€ 2. Merge Adapters into Final Model

```bash
loft merge \
  --base_model TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --adapter_dir adapter/adapter_v1 \
  --output_dir merged_models
```

> Produces a single merged HF model with integrated adapter weights.

---

## ğŸª„ 3. Export & Quantize to GGUF

```bash
# Export to GGUF format
loft export \
  --output_dir merged_models \
  --format gguf \
  merged_models

# Quantize to 4-bit GGUF (Q4_0)
loft quantize \
  --model_path merged_models/merged_models.gguf \
  --output_path merged_models/merged_models_q4.gguf \
  --quant_type Q4_0
```
âœ… Uses llama.cpp's Python or compiled tools
âœ… Output can be used directly with llama.cpp CLI
> Requires [llama.cpp](https://github.com/ggerganov/llama.cpp) â€” clone & build using `make`

---

## ğŸ’» 4. Inference with CLI Chat

```bash
loft chat \
  --model_path merged_models/merged_models_q4.gguf \
  --prompt "How do I bake a chocolate cake from scratch?" \
  --n_tokens 200
```

> Runs under 1GB RAM. Fast inference on MacBook/CPU. No GPU needed.

ğŸ“Š Benchmarks (MacBook Air, 8GB RAM)

| Step     | Output                   | Size   | Peak RAM | Time Taken |
| -------- | ------------------------ | ------ | -------- | ---------- |
| Finetune | Adapter (`.safetensors`) | 4.3 MB | 308 MB   | 23 min     |
| Merge    | Merged Model             | 4.2 GB | 322 MB   | 4.7 min    |
| Export   | GGUF (F32/FP16)          | 2.1 GB | 322 MB   | 83 sec     |
| Quantize | GGUF (Q4\_0)             | 607 MB | 322 MB   | 21 sec     |
| Chat     | Response @ 6.9 tok/s     | â€”      | 322 MB   | 79 sec     |

âš ï¸ Dataset: 20-sample Dolly-style JSON
ğŸ§ª Also tested on 300 samples (2 epochs = 1.5 hours)
âš ï¸ Note: The 300-sample run is a proof-of-concept to validate CPU-only finetuning.  
For production-quality adapters, larger datasets and GPU training will be recommended.

---


## ğŸ“ Project Structure

```bash
LoFT_v1/
â”œâ”€â”€ loft/                  # Core CLI code
â”‚   â”œâ”€â”€ cli.py             # CLI parser and dispatcher
â”‚   â”œâ”€â”€ train.py           # Finetuning logic
â”‚   â”œâ”€â”€ merge.py           # Adapter merge logic
â”‚   â”œâ”€â”€ export.py          # GGUF/ONNX export logic
â”‚   â””â”€â”€ chat.py            # CLI chat interface (WIP)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_finetune_data.json  # Sample dataset
â”œâ”€â”€ adapter/
â”‚   â””â”€â”€ adapter_v1/        # Output LoRA adapter files
â”œâ”€â”€ merged_models/
â”‚   â”œâ”€â”€ merged_models.gguf         # Exported GGUF model
â”‚   â”œâ”€â”€ merged_models_q4.gguf      # Quantized model (Q4_0)
â”œâ”€â”€ llama.cpp/             # Cloned llama.cpp directory (user must build)
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ .gitignore
```

---

## ğŸ“š Sample Training Data Format

```json
[
  {
    "instruction": "Who were the children of the legendary Garth Greenhand, the High King of the First Men in the series A Song of Ice and Fire?",
    "input": "",
    "output": "Garth the Gardener, John the Oak, Gilbert of the Vines, Brandon of the Bloody Blade..."
  },
  {
    "instruction": "Give me a list of basic ingredients for baking cookies",
    "input": "",
    "output": "Flour, sugar, eggs, milk, butter, baking powder, chocolate chips, cinnamon..."
  }
]
```

---

## ğŸ› ï¸ Requirements

* Python 3.10+
* `transformers`, `peft`, `datasets`, `accelerate`
* llama.cpp (for quantization & inference)
* Optional: `bitsandbytes` (for 4-bit training)

---

## ğŸ—ºï¸ Roadmap

* [x] Local LoRA finetuning CLI
* [x] Merge + GGUF Export
* [x] Quantization (Q4/Q8)
* [x] Local CPU Inference
* [ ] Gradio UI for LoFT Chat
* [ ] ONNX Export support
* [ ] SaaS dashboard for inference cost
* [ ] Adapter Marketplace

---

## ğŸªª License

MIT License â€” free to use, modify, and distribute.

---

## ğŸŒ Author

Built by [@diptanshukumar](https://www.linkedin.com/in/diptanshu-kumar) â€” strategy consultant turned AI builder. Contributions welcome!
